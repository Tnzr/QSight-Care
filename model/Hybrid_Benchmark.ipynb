{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f2d205",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "- Load the hybrid checkpoint and rebuild the evaluation data split.\n",
    "- Compare classical-only vs hybrid masks across accuracy, latency, and confusion matrices.\n",
    "- Capture sample confidence visualisations to support benchmarking and publication figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b6f72",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "- Load the hybrid checkpoint and rebuild the evaluation data split.\n",
    "- Compare classical-only vs hybrid masks across accuracy, latency, and confusion matrices.\n",
    "- Capture sample confidence visualisations to support benchmarking and publication figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime dependencies (no-op if already present)\n",
    "!pip install -q kagglehub qiskit qiskit_machine_learning > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2efb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from hybrid_components import (\n",
    "    DiabeticRetinopathyDataset,\n",
    "    ClassicalDRModel,\n",
    "    tensor_to_numpy_image,\n",
    "    create_confusion_plot,\n",
    "    create_head_confidence_figure,\n",
    "    generate_saliency_overlay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"dataset\": \"sovitrath\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 2,\n",
    "    \"val_fraction\": 0.2,\n",
    "    \"split_seed\": 42,\n",
    "    \"checkpoint_path\": \"complete_checkpoint.pth\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"class_mask_scenarios\": {\n",
    "        \"classical_only\": [1, 1, 0],\n",
    "        \"hybrid_full\": [1, 1, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "EVAL_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, targets):\n",
    "    return (preds == targets).sum() / max(len(targets), 1)\n",
    "\n",
    "def evaluate_with_mask(model, dataloader, device, mask, scenario_name, class_names):\n",
    "    model.eval()\n",
    "    mask_tensor = torch.tensor(mask, device=device, dtype=torch.float32)\n",
    "    total_samples = 0\n",
    "    correct_counts = {\"Classical A\": 0, \"Classical B\": 0, \"Quantum\": 0, \"Ensemble\": 0}\n",
    "    latencies = {\"classical_a\": 0.0, \"classical_b\": 0.0, \"quantum\": 0.0, \"ensemble\": 0.0}\n",
    "    predictions = {\"Classical A\": [], \"Classical B\": [], \"Quantum\": [], \"Ensemble\": []}\n",
    "    targets = []\n",
    "    sample_visuals = []\n",
    "    for batch_idx, (images, labels, paths) in enumerate(tqdm(dataloader, desc=f'Evaluating [{scenario_name}]')):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, return_all=True, active_mask=mask_tensor)\n",
    "        prob_a = F.softmax(outputs['output_a'], dim=1)\n",
    "        prob_b = F.softmax(outputs['output_b'], dim=1)\n",
    "        prob_c = F.softmax(outputs['output_c'], dim=1)\n",
    "        prob_ensemble = F.softmax(outputs['final_output'], dim=1)\n",
    "        pred_a = torch.argmax(outputs['output_a'], dim=1)\n",
    "        pred_b = torch.argmax(outputs['output_b'], dim=1)\n",
    "        pred_c = torch.argmax(outputs['output_c'], dim=1)\n",
    "        pred_ensemble = torch.argmax(outputs['final_output'], dim=1)\n",
    "        total_samples += labels.size(0)\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "        predictions['Classical A'].extend(pred_a.cpu().numpy().tolist())\n",
    "        predictions['Classical B'].extend(pred_b.cpu().numpy().tolist())\n",
    "        predictions['Quantum'].extend(pred_c.cpu().numpy().tolist())\n",
    "        predictions['Ensemble'].extend(pred_ensemble.cpu().numpy().tolist())\n",
    "        if mask_tensor[0] > 0:\n",
    "            correct_counts['Classical A'] += (pred_a == labels).sum().item()\n",
    "        if mask_tensor[1] > 0:\n",
    "            correct_counts['Classical B'] += (pred_b == labels).sum().item()\n",
    "        if mask_tensor[2] > 0:\n",
    "            correct_counts['Quantum'] += (pred_c == labels).sum().item()\n",
    "        correct_counts['Ensemble'] += (pred_ensemble == labels).sum().item()\n",
    "        for key in latencies:\n",
    "            latencies[key] += outputs['latencies'].get(key, 0.0)\n",
    "        if batch_idx == 0:\n",
    "            sample_count = min(2, images.size(0))\n",
    "            for idx in range(sample_count):\n",
    "                head_probs = {\n",
    "                    'Classical A': prob_a[idx].detach().cpu().numpy(),\n",
    "                    'Classical B': prob_b[idx].detach().cpu().numpy(),\n",
    "                    'Quantum': prob_c[idx].detach().cpu().numpy(),\n",
    "                    'Ensemble': prob_ensemble[idx].detach().cpu().numpy()\n",
    "                }\n",
    "                sample_visuals.append({\n",
    "                    'image': images[idx].detach().cpu(),\n",
    "                    'head_probs': head_probs,\n",
    "                    'target': labels[idx].item(),\n",
    "                    'predictions': {\n",
    "                        'Classical A': pred_a[idx].item(),\n",
    "                        'Classical B': pred_b[idx].item(),\n",
    "                        'Quantum': pred_c[idx].item(),\n",
    "                        'Ensemble': pred_ensemble[idx].item()\n",
    "                    }\n",
    "                })\n",
    "    num_batches = max(len(dataloader), 1)\n",
    "    latencies = {key: value / num_batches for key, value in latencies.items()}\n",
    "    targets_np = np.array(targets)\n",
    "    results = {\n",
    "        'acc': {\n",
    "            'a': float(correct_counts['Classical A'] / total_samples) if mask_tensor[0] > 0 else float('nan'),\n",
    "            'b': float(correct_counts['Classical B'] / total_samples) if mask_tensor[1] > 0 else float('nan'),\n",
    "            'c': float(correct_counts['Quantum'] / total_samples) if mask_tensor[2] > 0 else float('nan'),\n",
    "            'ensemble': float(correct_counts['Ensemble'] / total_samples) if total_samples > 0 else 0.0\n",
    "        },\n",
    "        'latencies': latencies,\n",
    "        'confusion': {\n",
    "            'Classical A': confusion_matrix(targets_np, predictions['Classical A'], labels=list(range(len(class_names)))) if mask_tensor[0] > 0 else None,\n",
    "            'Classical B': confusion_matrix(targets_np, predictions['Classical B'], labels=list(range(len(class_names)))) if mask_tensor[1] > 0 else None,\n",
    "            'Quantum': confusion_matrix(targets_np, predictions['Quantum'], labels=list(range(len(class_names)))) if mask_tensor[2] > 0 else None,\n",
    "            'Ensemble': confusion_matrix(targets_np, predictions['Ensemble'], labels=list(range(len(class_names))))\n",
    "        },\n",
    "        'report': classification_report(targets_np, predictions['Ensemble'], target_names=class_names, zero_division=0),\n",
    "        'sample_visuals': []\n",
    "    }\n",
    "    for sample in sample_visuals:\n",
    "        saliency_fig = generate_saliency_overlay(model, sample['image'], sample['predictions']['Ensemble'], device, mask_tensor)\n",
    "        sample['saliency_figure'] = saliency_fig\n",
    "        results['sample_visuals'].append(sample)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2df306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint and prepare dataloader\n",
    "checkpoint_path = Path(CONFIG['checkpoint_path'])\n",
    "if not checkpoint_path.exists():\n",
    "    raise FileNotFoundError(f'Checkpoint not found: {checkpoint_path}')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=CONFIG['device'])\n",
    "model_info = checkpoint['model_info']\n",
    "model = ClassicalDRModel(\n",
    "    encoder_type=model_info['encoder_type'],\n",
    "    num_classes=model_info['num_classes'],\n",
    "    compressed_dim=model_info['compressed_dim']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(CONFIG['device'])\n",
    "class_names = model_info.get('classes', [str(i) for i in range(model_info['num_classes'])])\n",
    "model.classes = class_names\n",
    "model.enable_latency_tracking(True)\n",
    "model.eval()\n",
    "dataset = DiabeticRetinopathyDataset(dataset_type=CONFIG['dataset'], mode='benchmark', transform=EVAL_TRANSFORM)\n",
    "val_size = int(CONFIG['val_fraction'] * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "if val_size == 0:\n",
    "    val_size = len(dataset)\n",
    "    train_size = 0\n",
    "generator = torch.Generator().manual_seed(CONFIG['split_seed'])\n",
    "if train_size > 0:\n",
    "    _, val_subset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "else:\n",
    "    val_subset = dataset\n",
    "val_loader = DataLoader(val_subset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "print(f'Validation samples: {len(val_subset)}')\n",
    "scenario_metrics = {}\n",
    "rows = []\n",
    "for scenario_name, mask in CONFIG['class_mask_scenarios'].items():\n",
    "    metrics = evaluate_with_mask(model, val_loader, CONFIG['device'], mask, scenario_name, class_names)\n",
    "    scenario_metrics[scenario_name] = metrics\n",
    "    rows.append({\n",
    "        'scenario': scenario_name,\n",
    "        'ensemble_accuracy': metrics['acc']['ensemble'],\n",
    "        'classical_a_accuracy': metrics['acc']['a'],\n",
    "        'classical_b_accuracy': metrics['acc']['b'],\n",
    "        'quantum_accuracy': metrics['acc']['c'],\n",
    "        'latency_classical_a_ms': metrics['latencies']['classical_a'] * 1e3,\n",
    "        'latency_classical_b_ms': metrics['latencies']['classical_b'] * 1e3,\n",
    "        'latency_quantum_ms': metrics['latencies']['quantum'] * 1e3\n",
    "    })\n",
    "results_df = pd.DataFrame(rows)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "for scenario_name, metrics in scenario_metrics.items():\n",
    "    print(f'=== {scenario_name} ===')\n",
    "    for head_name, cm in metrics['confusion'].items():\n",
    "        if cm is None:\n",
    "            continue\n",
    "        fig = create_confusion_plot(cm, class_names, f'{scenario_name} - {head_name}')\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "    print(metrics['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dbffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample confidence and saliency views\n",
    "for scenario_name, metrics in scenario_metrics.items():\n",
    "    samples = metrics.get('sample_visuals', [])\n",
    "    if not samples:\n",
    "        continue\n",
    "    sample = samples[0]\n",
    "    title = f\"{scenario_name} | target={class_names[sample['target']]}\"\n",
    "    fig = create_head_confidence_figure(sample['image'], sample['head_probs'], class_names, title)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "    if sample.get('saliency_figure') is not None:\n",
    "        display(sample['saliency_figure'])\n",
    "        plt.close(sample['saliency_figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install runtime dependencies (no-op if already present)\n",
    "!pip install -q kagglehub qiskit qiskit_machine_learning > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438503f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import kagglehub\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"dataset\": \"sovitrath\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 2,\n",
    "    \"val_fraction\": 0.2,\n",
    "    \"split_seed\": 42,\n",
    "    \"checkpoint_path\": \"complete_checkpoint.pth\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"class_mask_scenarios\": {\n",
    "        \"classical_only\": [1, 1, 0],\n",
    "        \"hybrid_full\": [1, 1, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "EVAL_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions (mirrors training notebook)\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, encoder_type='vit', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder_type = encoder_type\n",
    "        if encoder_type == 'vit':\n",
    "            self.encoder = models.vit_b_16(pretrained=pretrained)\n",
    "            self.encoder.heads = nn.Identity()\n",
    "            self.projection = nn.Linear(768, 2048)\n",
    "        else:\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "            self.encoder = nn.Sequential(\n",
    "                resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
    "                resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4,\n",
    "                nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()\n",
    "            )\n",
    "            self.projection = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        if self.encoder_type == 'vit':\n",
    "            features = self.projection(features)\n",
    "        return features\n",
    "\n",
    "class CompressionModule(nn.Module):\n",
    "    def __init__(self, input_dim=2048, compressed_dim=30):\n",
    "        super().__init__()\n",
    "        self.compressor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, compressed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.compressor(x)\n",
    "\n",
    "class ClassicalHeadA(nn.Module):\n",
    "    def __init__(self, input_dim=2048, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class ClassicalHeadB(nn.Module):\n",
    "    def __init__(self, input_dim=30, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class QuantumClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim=32, num_classes=5, num_qubits=4, shots=1024):\n",
    "        super().__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_classes = num_classes\n",
    "        self.q_device = torch.device('cpu')\n",
    "        self.input_projection = nn.Linear(input_dim, num_qubits)\n",
    "        feature_map = ZZFeatureMap(num_qubits, reps=2)\n",
    "        ansatz = RealAmplitudes(num_qubits, reps=2)\n",
    "        circuit = feature_map.compose(ansatz)\n",
    "        sampler = Sampler(options={'shots': shots})\n",
    "        self.qnn = SamplerQNN(\n",
    "            sampler=sampler,\n",
    "            circuit=circuit,\n",
    "            input_params=feature_map.parameters,\n",
    "            weight_params=ansatz.parameters,\n",
    "            sparse=False,\n",
    "            input_gradients=True\n",
    "        )\n",
    "        initial_weights = torch.zeros(self.qnn.num_weights, dtype=torch.double)\n",
    "        self.q_layer = TorchConnector(self.qnn, initial_weights=initial_weights)\n",
    "        output_dim = 2 ** num_qubits\n",
    "        self.post_process = nn.Sequential(\n",
    "            nn.Linear(output_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        projected = self.input_projection(x)\n",
    "        projected_cpu = projected.to(self.q_device, dtype=torch.double)\n",
    "        quantum_raw = self.q_layer(projected_cpu)\n",
    "        quantum_raw = quantum_raw.to(dtype=torch.float32)\n",
    "        target_device = projected.device\n",
    "        if any(True for _ in self.post_process.parameters()):\n",
    "            target_device = next(self.post_process.parameters()).device\n",
    "        quantum_raw = quantum_raw.to(target_device)\n",
    "        logits = self.post_process(quantum_raw)\n",
    "        return logits\n",
    "\n",
    "class DynamicEnsemble(nn.Module):\n",
    "    def __init__(self, num_heads=3, init_temp=1.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.base_weights = nn.Parameter(torch.ones(num_heads) / num_heads)\n",
    "        self.temperature = nn.Parameter(torch.tensor(init_temp))\n",
    "        self.uncertainty_scales = nn.Parameter(torch.ones(num_heads))\n",
    "    def forward(self, head_outputs, uncertainties=None, active_mask=None):\n",
    "        if active_mask is None:\n",
    "            mask = torch.ones(len(head_outputs), device=head_outputs[0].device)\n",
    "        else:\n",
    "            mask = active_mask.to(head_outputs[0].device).float()\n",
    "        weights = F.softmax(self.base_weights / self.temperature, dim=0)\n",
    "        weights = weights * mask\n",
    "        if weights.sum() <= 0:\n",
    "            weights = torch.ones_like(weights) * mask\n",
    "        weights = weights / (weights.sum() + 1e-8)\n",
    "        if uncertainties is not None:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
